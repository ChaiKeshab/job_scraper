# ğŸ§  Job Tracker Project Plan

A complete local-first setup to collect, manage, and visualize job listings scraped from the web.

---

## ğŸ§± Phase 1: Database Design & Setup

We started by identifying the core problem: storing scraped job data in a structured and reliable way. Instead of relying on temporary JSON files or CSV exports, we decided to use **PostgreSQL** locally.

To interact with it efficiently, we use **Drizzle ORM** â€” a lightweight and typesafe way to manage our database schema.

### âœ… Outcome

* Created normalized tables for:

  * Companies
  * Jobs
  * Contacts
  * Applications
  * Files (resumes, cover letters)
  * Notes
  * Tags (with many-to-many relation via job_tags)
* Our data storage is fully ready to handle relational queries.

This allows us to not only store raw scraped data but also link it to applications, notes, and used files â€” basically, a full job tracking system.

---

## ğŸ•¸ï¸ Phase 2: Data Scraping System

Next step: getting actual data into our database.

We will build a scraping pipeline using **Next.js (API routes)** integrated with **Cheerio** and **Playwright**.

### Plan

1. **Cheerio** will handle static HTML scraping â€” perfect for parsing job boards or LinkedIn search pages.
2. **Playwright** will handle dynamic pages that require JavaScript rendering (e.g., sites that load listings via client-side JS).
3. Scraped data (company, job title, location, URL, etc.) will be sanitized and stored directly into our PostgreSQL tables using Drizzle ORM.
4. We'll include a lightweight logging mechanism to keep track of scrape sessions, errors, and duplicate checks.

### âœ… Outcome

* Automated scripts or API routes that can scrape, clean, and insert data into our DB.
* Flexibility to trigger scrapes manually or via a Next.js button later.

---

## ğŸ’¾ Phase 3: File Management System

Since we are running locally, cloud storage (AWS, Supabase, etc.) isn't in use yet. We'll maintain all file references locally.

### Plan

* Store resumes and cover letters inside a `/public/files/` directory in the Next.js project.
* Each entry in the `files` table will store metadata such as filename, type (resume/cover_letter), and relative path.
* Future migration to cloud storage will be simple since file paths and metadata are already modeled.

### âœ… Outcome

* Simple and organized local file management tied to database records.

---

## ğŸ§® Phase 4: Data Visualization & Testing UI

Before going into automation or dashboard-level features, weâ€™ll create a **basic UI** to test and view our data.

### Plan

1. Use a simple **Next.js page** that queries PostgreSQL through Drizzle.
2. Display the list of scraped jobs â€” including company name, title, and status.
3. Provide a minimal filtering and search feature for testing.
4. Eventually, we can build components for job detail view, file usage tracking, and application history.

### âœ… Outcome

* A minimal frontend confirming our backend setup works correctly.
* Foundation for future admin-like dashboard.

---

## ğŸš€ Phase 5: Migration & Automation Plan

Once we confirm local development stability:

1. Containerize PostgreSQL with Docker for portability.
2. Set up a job queue system (later) for periodic scrapes.
3. Optionally, migrate storage to Supabase or a remote PostgreSQL instance.

### âœ… Long-term Goal

A fully automated job tracking system with a personal dashboard showing:

* All scraped listings
* Companies contacted
* Files used
* Application progress
* Notes & follow-ups

---

## ğŸ—‚ï¸ Final Project Folder Structure (No `/src` directory)

```
project-root/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ page.tsx                      # Homepage or dashboard
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ page.tsx                  # Display all scraped jobs
â”‚   â”‚   â”œâ”€â”€ [id]/page.tsx             # Job detail view
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ scrape/
â”‚   â”‚   â”‚   â”œâ”€â”€ route.ts              # API route for scraping trigger
â”‚   â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”‚   â”œâ”€â”€ route.ts              # CRUD API for job records
â”‚   â”‚   â”œâ”€â”€ files/
â”‚   â”‚   â”‚   â”œâ”€â”€ route.ts              # Upload/read local files
â”‚   â”‚   â”œâ”€â”€ companies/
â”‚   â”‚   â”‚   â”œâ”€â”€ route.ts              # CRUD API for companies
â”‚   â”‚   â”œâ”€â”€ notes/
â”‚   â”‚   â”‚   â”œâ”€â”€ route.ts              # Manage notes linked to jobs
â”‚   â””â”€â”€ components/
â”‚       â”œâ”€â”€ JobCard.tsx               # Display job summary
â”‚       â”œâ”€â”€ JobTable.tsx              # Table listing of jobs
â”‚       â”œâ”€â”€ FileUploader.tsx          # Handle file links locally
â”‚       â”œâ”€â”€ Layout.tsx
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ schema.ts                     # Drizzle ORM schemas
â”‚   â”œâ”€â”€ drizzle.config.ts             # Drizzle configuration
â”‚   â”œâ”€â”€ migrations/                   # Auto-generated migration files
â”‚   â””â”€â”€ index.ts                      # Drizzle client setup
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scrapeStatic.ts               # Cheerio-based scraper
â”‚   â”œâ”€â”€ scrapeDynamic.ts              # Playwright-based scraper
â”‚   â”œâ”€â”€ seedData.ts                   # Seed JSON/local data into DB
â”‚
â”œâ”€â”€ public/
â”‚   â””â”€â”€ files/                        # Local resumes and cover letters
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ dataFormatter.ts              # Cleans and formats scraped data
â”‚   â”œâ”€â”€ dedupe.ts                     # Duplicate job check helper
â”‚   â”œâ”€â”€ constants.ts
â”‚
â”œâ”€â”€ package.json
â”œâ”€â”€ .env.local                        # Local DB connection string
â””â”€â”€ README.md
```

---

## ğŸ§© Summary of Progress

| Phase | Description                                          | Status     |
| ----- | ---------------------------------------------------- | ---------- |
| 1     | Database schema design (PostgreSQL + Drizzle)        | âœ… Done     |
| 2     | Scraping system (Cheerio + Playwright + Next.js API) | ğŸ”œ Next    |
| 3     | Local file management                                | ğŸ”œ Next    |
| 4     | UI for viewing DB data                               | ğŸ”œ Pending |
| 5     | Migration & Automation                               | ğŸ•’ Later   |

---

This document will be updated as we move through each phase. The idea is to keep the setup minimal, local, and easy to migrate to cloud later.
